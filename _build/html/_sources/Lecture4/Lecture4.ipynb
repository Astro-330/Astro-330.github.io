{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4. Sampling with MCMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What, exactly, is a sampler?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you asked me a while ago what MCMC was, I might have answered that it is a tool for fitting models to data. And while it's true that MCMC is good for this general task, known as *inference*, we can actually take a step back and understand Monte Carlo schemes from a more basic standpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*MCMC is a method for solving integrals.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let me break that down a bit more. MCMC is a *sampling algorithm*. It generates samples from what we refer to as a *posterior*, but for the moment we can simply think of it as some function. By sampling, I mean the most naive thing possible --- like drawing balls from a bucket. If I define some function $f(x)$, and I start evaluating $f(x)$ at various points $x_i$, that is a sampling. What makes what we're going to do here more special is the statistical properties of those samples, for the problems at hand. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solving integrals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In some sense, the only thing that MCMC is truly meant for is sampling pdfs (probability density functions). But that sounds super abstract. So let's think about this in more real terms. Let's say I want to integrate a function, \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "I = \\int_{a}^{b}f(x)dx\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If I asked you to integrate $f(x)$, what would you do? Well, it depends on what $f(x)$ is, right? If "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "f(x) = x^2, \n",
    "$$\n",
    "you would simply tell me that \n",
    "\n",
    "$$\n",
    "I = \\int_{a}^{b}f(x)dx = \\int_{a}^{b}x^2 dx = \\frac{b^3}{3} - \\frac{a^3}{3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's imagine that $f(x)$ is *ugly*. Ugly like one of those functions you derive halfway through a problem on a Physics midterm and you realize something must not be right because there's *no way* you can integrate *that* ugly expression on this test (or indeed, ever). \n",
    "\n",
    "\n",
    "What then?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Well, usually the answer would be either \"Wolfram Alpha\" or, more generally, \"Numerical Integration\". Numerical integration says, \"I can estimate the area under this complex curve by chunking it into finite rectangles/trapazoids/etc. and then calculate a sum\". You've probably heard of (or used some of these methods): midpoint rule, trapezoidal rule, simpsons rule, Gaussian quadrature... (many of these are implemented in the `scipy.integrate` module)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When you're dealing with a (relatively) well behaved function in one dimension, those methods are often the way to go (and the first thing we jump to in our code). But what happens if our problem is not one dimensional? What if, for example, $f$ is a function of three spatial quantities and three additional parameters,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "f(\\theta) = f(x,y,z,a,b,c)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We now have $\\theta$ as a vector of six parameters, meaning our integral looks more like \n",
    "\n",
    "$$\n",
    "I = \\int \\int \\int \\int \\int \\int f(x,y,z,a,b,c) dx\\hspace{1pt} dy\\hspace{1pt} dx\\hspace{1pt} da\\hspace{1pt} db\\hspace{1pt} dc\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can now ask ourselves, *Can our above numerical integration schemes handle this?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Each scheme above has an associated error, which comes from how the scheme is integrated. From Calculus, you probably remember that the trapezoid rule usually produces smaller errors than the midpoint rule, as it better approximates the curve being traced. We can actually write down how the error of each of these scales. I'll use the Trapezoid rule here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\epsilon \\propto \\frac{1}{N^{2/d}}\n",
    "$$\n",
    "where $N$ is the number of sample points (i.e., how fine our grid where we evaluate our trapezoid) and $d$ is the number of dimensions being integrated over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " This is a big problem. The error in our numerical solution to the integral scales to a power of the dimensions being integrated over, which requires us to have intractably large values of $N$ to get accurate results. This is often referred to as \"the curse of dimensionality.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So how do we get around this? \n",
    "\n",
    "What if instead of trying to \"grid up\" this multidimensional space and evaluate our function at each location, I simply \"threw a dart\" at a random location and evaluated it there? It turns out, you can show that the error in such a sampling method has an error of "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\epsilon \\propto \\frac{1}{N^{1/2}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Amazingly, this does not have any dependence on dimensionality! So doing a little math with the trapizoid-rule error above, we can see that for problems with dimensionality greater than ~$d=4$ (for this rule, and closer to $d=6-8$ for, e.g., Simpson's rule), the error properties of an MCMC algorithm win out, and make the integration tractable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## But how does the integral actually get computed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's back up for a moment to the 1D case of $f(x)$ to aid in our visualization. If I draw some arbitrary function $f(x)$ across my plot, I can evaluate the integral (area) by any of the tools above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "I could also sample, which in the absolute first order case means choosing random $\\sim U(a,b)$ (uniformly drawn) values over the bounds of the integrand (i.e., in the 1D case here, values of $x$ between $a$ and $b$), and then evaluate $f(x)$ at those values. This is, quite literally, throwing darts to pick values (and where the method gets the Monte Carlo part of it's name)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Imagine I have my function $f(x)$ that looks like this\n",
    "\n",
    "<img src=\"fx.png\" width='400'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "My sampling, as I described it above, corresponds to something like\n",
    "\n",
    "<img src=\"fx_samples.png\" width='350'/>\n",
    "\n",
    "where the four points $x_i$ are presumed to have been drawn from some random uniform distribution. (so more likely, they will not be in ascending order of $x$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To estimate the area under the curve, I create a rectangle for each sample $f(x_i)$ with with a width of $(b-a)$ and a height of $f(x_i)$. For example, for $f(x_1)$ above, this would look like \n",
    "\n",
    "<img src=\"fx1_area.png\" width='350'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "while the rectangle for $f(x_3)$ would look like \n",
    "\n",
    "<img src=\"fx3_area.png\" width='360'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can see that sometimes I overestimate the area, and other times I underestimate it. However, I will claim here, and prove below, that the *expectation value* (i.e., the *average*) of all of these rectangles represents an accurate estimate of the integral of the function $f(x)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In short, I'm asserting for the moment that the expectation value by the normal integral, i.e., \n",
    "$$\n",
    "\\int f(x)p(x)dx.\n",
    "$$\n",
    "Is going to be approximated by \n",
    "$$\n",
    "E_{p(\\theta)}[f(\\theta)] =  \\int f(\\theta)p(\\theta)d\\theta \\approx \\frac{1}{K}\\sum_{k=1}^{K}\\frac{f(\\theta_k)}{p(\\theta_k)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's explain why. In the case of a Uniform distribution, we know that our $p(\\theta_k)$ is given by, simply \n",
    "\n",
    "$$\n",
    "p(\\theta_k) = \\frac{1}{b-a}\n",
    "$$\n",
    "\n",
    "That is, a uniform over some bounds is normalized at the level $1/(b-a)$ such that the area of the distribution is properly normalized to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Recall I computed my rectangle areas as the width $(b-a)$ times the height of the function at different sample locations. I'm thus approximating my integral as "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "I \\approx \\frac{1}{K}\\sum_{k=1}^{K}f(x_k)(b-a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Notice though, that $(b-a)$ is just $1 / p(x)$ as we've defined it. Thus we can write our sum as we did above. But why does that work? I.e., can we show that this formula actually esimates the integral? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at the *expectation value* of the estimated integral $I$. Remember, every time I use my MCMC estimator above, I'll get a somewhat different answer because I drew random points. What we want is the mean value of many estimates of the integral, $\\langle I\\rangle$, to *be* the integral's value given enough samples. This is something I can show."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The expectation value for $I$, by the normal formula, is given by \n",
    "\n",
    "$$\n",
    "\\langle I \\rangle = \\int I(x)p(x)dx = \\int\\left[ \\frac{1}{K}\\sum_{k=1}^{K}\\left(\\frac{f(x_k)}{p(x_k)}\\right)\\right]p(x) dx=E\\left[ \\frac{1}{K}\\sum_{k=1}^{K}\\left(\\frac{f(x_k)}{p(x_k)}\\right)\\right]\n",
    "$$\n",
    "plugging in the expression for $I$ that I asserted above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By Lebesgue's dominated convergence theorem, (in the limit as K goes to $\\infty$), we can move the expectation value inside the sum, such that \n",
    "$$\n",
    "\\langle I \\rangle = E\\left[ \\frac{1}{K}\\sum_{k=1}^{K}\\left(\\frac{f(x_k)}{p(x_k)}\\right)\\right] \n",
    "$$\n",
    "$$\n",
    "= \\frac{1}{K}\\sum_{k=1}^{K}E\\left[\\left(\\frac{f(x_k)}{p(x_k)}\\right)\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "where since the expectation value for any particular $f(x_k)/p(x_k)$ does not depend on $k$, and is just the expectation value over the region, we can pull it out of the sum:\n",
    "$$\n",
    "= E\\left[\\frac{f(x)}{p(x)}\\right]\\frac{1}{K}\\sum_{k=1}^{K} 1\n",
    "$$\n",
    "$$\n",
    "=E\\left[\\frac{f(x)}{p(x)}\\right]\\frac{1}{K}K\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "= E\\left[\\frac{f(x)}{p(x)}\\right]\n",
    "$$\n",
    "which, by the definition of expectation values, is just \n",
    "$$\n",
    " = \\int \\frac{f(x)}{p(x)}p(x) dx \n",
    "$$\n",
    "$$\n",
    "\\langle I \\rangle = \\int f(x)dx \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It confused me for quite some time to think about what the expectation value on some quantity $f(x_i)/p(x_i)$ looks like, as these are just numbers. But, recall, we are talking about the expectation value $\\langle I \\rangle$, which is computed over many *simulations* of I (i.e., running our sampler many times). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Thinking about it this way, we can see that the first term in our sum, for example, \n",
    "\n",
    "$$\n",
    "\\left[\\frac{f(x_0)}{p(x_0)}\\right]\n",
    "$$\n",
    "will be *different* every time I run the sampler (since the $x_0$ is a randomly generated number)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Thus the value inside this expecation can take *any* value allowed on $f(x)$ given the set definite boundaries. It then becomes more clear that for this particular term in the sum, the expectation value *must* just be the expectation value of the function over the bounds. This is then true for *every* term in the sum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Of course, we've based this derivation on a limit as $K\\rightarrow\\infty$, but in reality we are taking finite numbers of samples. There thus raises a question of \"how many samples are needed for my approximation to be accurate?\" This gets either into deep mathematics or pure heuristics, so I'll simply say for now that we take as many samples as is feasible, and in general if we have many independent samples we are doing O.K."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It's useful to point out here that when doing *inference* problems, we're often trying to integrate something that looks like the expectation value above, i.e., the integral of a likelihood times a prior. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the simplest case (Monte Carlo) we simply draw random (uniform) values of $\\theta$ and compute the expectation value using the sum. We then use that expectation value, and the bounds of our integral, to solve for the area. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For example, let's take \n",
    "\n",
    "$$\n",
    "f(x) = x^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "and I want to integrate from 1 to 2, \n",
    "\n",
    "$$\n",
    "I = \\int_{1}^{2}x^2 dx\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Obviously we know the answer to this is $8/3 - 1/3 = 7/3$. Let's solve it using Monte Carlo:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "integral as estimated from 1000 Samples: 2.3081404190522363\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "unif = stats.uniform(1,1) #this creates a uniform over the range [1,2]\n",
    "\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "sample_sum = 0\n",
    "N = 1000\n",
    "for i in range(N):\n",
    "    s = unif.rvs()\n",
    "    call = f(s)\n",
    "    sample_sum += call\n",
    "    \n",
    "sample_sum /= N\n",
    "print(\"integral as estimated from 1000 Samples: {}\".format(sample_sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We know that the true value is 2.33 repeating, here we can see that with 1000 samples, we estimate the integral to be 2.32. \n",
    "\n",
    "We can also try with a much (somewhat absurdly) higher N:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "integral as estimated from 100,000 Samples: 2.329695672498891\n"
     ]
    }
   ],
   "source": [
    "N = 100000\n",
    "for i in range(N):\n",
    "    s = unif.rvs()\n",
    "    call = f(s)\n",
    "    sample_sum += call\n",
    "    \n",
    "sample_sum /= N\n",
    "print(\"integral as estimated from 100,000 Samples: {}\".format(sample_sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can see that in this case we're very close, with the trailing digits rounding effectively to 2.33. \n",
    "\n",
    "I mentioned above that the error in our estimate of the integral in this Monte Carlo scheme scaled as $\\propto N^{-1/2}$. We can write this more formally as \n",
    "\n",
    "$$\n",
    "\\epsilon = kN^{-1/2}\n",
    "$$\n",
    "\n",
    "where $k$ is a constant that captures the normalization of our scaling relation. Our goal at this point is to bring $k$ down as much as possible, so that our scaling with error has a lower normalization. (In the parlance of the expectation value $\\langle I \\rangle$ above, we want to reduce the *variance* in the estimates of I for any given sampling run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "## Importance Sampling \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you have a distribution that looks something like a Gaussian, defined at some range, like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def f2(x):\n",
    "    out = 3 * np.exp(-(x-5.)**2/(2*0.5**2))\n",
    "    return out\n",
    "\n",
    "xx = np.linspace(0,10,1000)\n",
    "y = f2(xx)\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax.plot(xx,y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "I could sample this function using a $\\sim U(0,10)$. But many of my samples would be \"wasted\" because they would be sampling regions (like between 0 and 3, or 8 and 10) where the value of $f(x)$ is very small, and thus the contribution to the integral is negligable. What if I had a way to throw darts that were more likely to land near 5, where I want to be well-sampled, and not as much near 10?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In order to improve my $k$ value, and assign some *importance* to some values of $\\theta$ (in this case x) to sample over others, I need a new probability distribution to sample from that isn't just the Uniform. Thinking about this for a moment, it would seem like the obvious choice is in fact, $f(x)$ itself, (or rather, f(x) normalized such that it is a probability density function). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This would naturally capture what I want to do: where f(x) is larger, the pdf will be larger, and the chance of drawing values there will be larger than elsewhere were f(x) is smaller. In this case, instead of a pdf that is just $1/(b-a)$, we will plug a real pdf into our sampling expression:\n",
    "\n",
    "$$\n",
    "\\int g(\\theta)p(\\theta)d\\theta \\approx \\frac{1}{K}\\sum_{k=1}^{K}\\frac{g(\\theta_k)}{p(\\theta_k)}\n",
    "$$\n",
    "\n",
    "Let's try setting up a problem using a Gaussian like above, and sample from a pdf that is the gaussian itself. \n",
    "- I'll set up my \"arbitrary\" function to return something that is gaussian shaped, but arbitrarily normalized. \n",
    "- I then set my \"pdf\" distribution to be a true, normalized normal distribution at the same ($\\mu,\\sigma$) (if we don't know these values, we can approximate them). \n",
    "- I repeat the exercise from before, normalizing each evaluation of my function by an evaluation of the proposal pdf at the same value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated Area: 3.759942411946498\n"
     ]
    }
   ],
   "source": [
    "#f(x) is not normalized, it's just something with a gaussian form, as I've multiplied by a constant\n",
    "def f2(x):\n",
    "    return 3 * np.exp(-(x-5.)**2/(2*0.5**2))\n",
    "\n",
    "gauss = stats.norm(5,0.5) #this is my new p(theta)\n",
    "N=100000\n",
    "area = []\n",
    "for i in range(N):\n",
    "    val = gauss.rvs()\n",
    "    call = f2(val) / gauss.pdf(val)\n",
    "    area.append(call)\n",
    "    \n",
    "norm_area = np.sum(area) / N\n",
    "\n",
    "print('Calculated Area: {}'.format(norm_area))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We know analytically that the area we should get is \n",
    "\n",
    "$$\n",
    "\\int_{-\\infty}^{\\infty} a e^{-(x-b)^{2} / 2 c^{2}} d x=a\\sqrt{2\\pi c^2}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where here, a is 3, b is 5, and c is 0.5. This gives me a computed analytical value of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7599424119465006"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "area_theoretical = np.sqrt(2*np.pi*0.5**2)*3\n",
    "area_theoretical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can see that once again we've gotten the answer almost exactly right. Note that this didn't only work because both my sampling distribution and pdf were Gaussians with different normalization. Any $f(x)$ that looked roughly like a \"bump\" could have been estimated this way. I simply chose a Gaussian because we could compare to an analytical solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now that we understand qualitatively how this process works with some simple 1D integrals, let's go back to thinking about ugly, multidimensional integrals. In the above situation, I was able to set a sampling distribution to be my target distribution because I knew the functional form of $f(x)$ completely. Now, if I knew it was a Gaussian but didn't know $(\\mu,\\sigma)$ I would have just run an optimizer on $f(x)$ first to find the maximum, and perhaps chosen a reasonably wide spread. \n",
    "\n",
    "But in the tougher cases, perhaps all I know is how to *evaluate* $f(\\theta)$ for some multidimensional vector $\\theta$, but know almost nothing about the details or shape of the distribution I'm trying to integrate. Above, I chose samples preferentially at higher likelihood because I knew ahead of time where those points would be. If I don't I can write an algorithm to sort that out for me. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Metropolis-Hastings MCMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We're now getting into the real meat of MCMC. I hope that taking the time to walk through the simpler cases above allows the following to be more clear!\n",
    "\n",
    "The Metropolis-Hastings algorithm alows you to create a **chain** of evaluations of your function, which don't depend on the initial conditions, but rather only on the evaluation immediately before. This biased \"walker\" is programmed to move loosely towards areas of higher probability, but occasionally will also move towards lower probability. This walker moves in \"steps\" that are usually a small sphere around its current location in parameter space. This allows us to very efficiently sample from the high-probability (or in terms of the integral, most important regions) *even if we don't have intimate knowledge of what that region looks like*. Our only requirement at this point is that we can evaluate our function at some position $\\theta$ and that our function as usual is positively defined over the bounds. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here is the schematic:\n",
    "\n",
    "- First, pick an initial value of $\\theta$ and evaluate it as above. Add this to a stored \"chain\" of values\n",
    "- Next, pick a $\\theta^\\prime$ from the *proposal pdf,* a pdf distribution centered on $\\theta$ (more on this below)\n",
    "- pick a number $r$ from a $Unif(0,1)$\n",
    "- if $f(\\theta^\\prime)/f(\\theta) > r$, then move to that position and add it to the chain\n",
    "- otherwise, the next position in the chain is set to be the current position (and it is added to the chain again)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do I mean by a proposal pdf? Our walker needs to know how to choose a step to take. The easiest, and most statistically simple, method for doing this is a Gaussian (multivariate if $\\theta$ is multivariate) with a mean of $\\mu=\\theta$ and some spread $\\sigma$ that is chosen for each given problem by the amount of parameter space being covered and how sharply $f$ varies. We'll discuss the exact choice of $\\sigma$ more later."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
